### Abstract
We offer a methodical analysis of three smoothing techniques for improving eigenvalue-weighted word embeddings on word similarity tasks. We compare performances across several common benchmarks using Spearman's rank correlation, and we examine the effects of the Jelinek--Mercer interpolation parameter lambda and the eigenvalue weighting factor on alignment with human similarity judgments. The efficiency of each method varies significantly depending on the dataset and weighting configuration, but empirically, Bayesian smoothing usually achieves the highest correlation, followed by Dirichlet and Jelinek--Mercer. Together with eigenvalue-based weighting, these results highlight the effectiveness of smoothing and point to interesting avenues for reducing data sparsity in word embedding challenges.
